# Sample config file for slurm.pl
#
# You must specify a 'command', and at the least rules for the 'gpu', 'mem' and
# 'num_threads' options, as Kaldi scripts sometimes add them. The rest is for
# your use, e. g. to tune the commands in recipe's cmd.sh.
command sbatch --no-kill --hint=compute_bound --export=PATH,TIME_STYLE,MILIEU

option debug=0
option debug=1
option debug=2 -v

# This is not very helpful as our Slurm setup does not schedule memory.
option mem=0
option mem=* --mem-per-cpu=$0

option num_threads=1
option num_threads=* --cpus-per-task=$0

# For memory-gobbling tasks, like large ARPA models or HCLG. Supporting other
# values but 1 makes no sense, as Slurm will allocate all these nodes to the
# job, but Kaldi does not use MPI or other communication mechanism to use more
# than a single-node jobs.
option whole_nodes=1 --exclusive --nodes=1

# You may add e.g. --gres=gcecpu:C2 if you have multiple node types to this case
option gpu=0
# --cpus-per-task should be 2*$0, but slurm.pl cannot do arithmetics. All our
# nodes are 1 GPU each anyway, so this is the only supported value; '--gpu 1' is
# hardcoded in train.py.
#
# Make sure to either remove 'p100' if you have only one GPU type defined, or
# select the correct type for the job. You can remove the --gres switch and
# create a separate option to specify the GPU type, but you'll need it pass it
# through train.py. Be sure to add --gres to the =* case of the option to
# specify the default type, otherwise if you fail to specify your option no
# --gres will be passed to slurm.pl, and Slurm will randomly mix GPU types; your
# training will run at the speed of the slowest one, while the fast expensive
# GPUs will have crunched their shard quickly and wait for the rest of the team.
option gpu=1 --partition=gpu --cpus-per-task=2 --gres=cuda:p100

# The --max-jobs-run option is supported as a special case by slurm.pl, because
# Slurm uses different syntax, not a switch. You don't need to handle it in the
# config file.
